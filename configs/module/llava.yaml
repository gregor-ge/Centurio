defaults:
  - default

model:
  _target_: src.modules.modeling.llava_module.LlavaModule
  vit_pretrained: ???
  lm_pretrained: ???
  compile: False
  load_8bit: False
  load_4bit: True
  use_flash_attn: True
  freeze_vit: True
  freeze_lm: True
  gradient_checkpoint: True
  use_lora: False
  lora_bias: none
  lora_alpha: 64
  lora_r: 32
  lora_dropout: 0.05
scheduler:
  _target_: transformers.optimization.get_cosine_schedule_with_warmup
  num_warmup_steps: 1000