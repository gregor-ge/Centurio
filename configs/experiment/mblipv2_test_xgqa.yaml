# @package _global_
defaults:
  - default
  - /dataspecs@datamodule.test:
      - xgqa_test #val
  - override /module: llava

run:
  seed: 42
  data_prefix: /media/gregor/DATA/projects/wuerzburg
  image_root: /media/gregor/DATA/projects/wuerzburg/iglue/datasets
  train_image_root: ${run.image_root}/llava_pretrain #/images
  train_data: ${run.data_prefix}/lvlm-early-eval/data
  train_file: llava_pretrain/llava_pretrain_converted.json # llava/llava_v1_5_mix665k_converted.json
  xm3600_image_root: ${run.image_root}/Crossmodal3600/images
  flickr_image_root: ${run.image_root}/flickr30k/flickr_images
  gqa_image_root: ${run.image_root}/gqa/images
  mme_image_root: ${run.image_root}/MME
  mmbench_image_root: ${run.image_root}/mmbench/images
  vstar_image_root: ${run.image_root}/vstar_bench
  imagenet_image_root: ${run.image_root}/imagenet/train
  textvqa_image_root: ${run.image_root}/textvqa/train_val_images/train_images

  vit_model: vit_large_patch14_clip_224.openai
  llm:  stabilityai/stablelm-2-zephyr-1_6b
  tokenizer_name: ${run.llm}
  test_padding_side: "left"

  test_batch_size: 16
  test_num_workers: 1
  train_batch_size: 16  # goal = 256 for pretrain; 128 finetune (used by others) but might be due to VRAM but lets keep follow that
  train_num_workers: 6

trainer:
#  limit_train_batches: 1.0
  limit_test_batches: 1.0
  max_epochs: 1
  devices: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  log_every_n_steps: 10
  val_check_interval: 0.5
  strategy: auto #deepspeed_stage_2
#    _target_: src.tasks.vllm.checkpoint.MyDeepSpeedStrategy
#    stage: 2
  limit_train_batches: 0.0
#  limit_val_batches: 5


module:
  _target_: src.modules.trident_module.llava_trident_module.LlavaTridentModule
  model:
#    _target_: src.modules.modeling.prune_lms.llava_module.LlavaModule
    lm_pretrained: ${run.llm}
    vit_pretrained: ${run.vit_model}
    train_checkpoint: /media/gregor/DATA/projects/wuerzburg/lvlm-early-eval/checkpoints/instruct/03_19_2024_15_34_08/checkpoints/0-4877.ckpt/checkpoint/mp_rank_00_model_states.pt
    lora_checkpoint: /media/gregor/DATA/projects/wuerzburg/lvlm-early-eval/checkpoints/instruct/03_19_2024_15_34_08/checkpoints/0-4877
    load_4bit: False
    load_8bit: False
    gradient_checkpoint: True
    use_flash_attn: False
    use_lora: True #True
    adapter_type: mlp  
    adapter_config:
      layers: 6
      ps_queries: 64
      multi_scale: "${oc.select:run.multiscale,2}"
    lora_alpha: 64
    lora_r: 32
    lora_dropout: 0.05
    overwrite_size: "${oc.select:run.overwrite_size,-1}"
#    prune_keep: 0.25
#    prune_mode: "pool"
  optimizer:
    lr: 0.001  # llava: 0.001  perceiver: 0.0005 honeybee: 0.0003 mobilevlm: 0.001
    weight_decay: 0.01  # llava: 0 perceiver 0.0001 honeybee 0.01 mobilevlm: 0
  scheduler:
    _target_: transformers.optimization.get_cosine_schedule_with_warmup
    num_warmup_steps: 0.03 # like llava

logger:
  csv:
    save_dir: ${hydra:runtime.output_dir}
  wandb:
    name: "test-xgqa_${hydra:runtime.output_dir}"
    project: "centurio"


callbacks:
  model_checkpoint_on_epoch:
    _target_: src.tasks.vllm.checkpoint.LlavaModelCheckpoint #lightning.pytorch.callbacks.ModelCheckpoint
    lora: ${module.model.use_lora}
#    monitor: "" # name of the logged metric which determines when model is improving
    every_n_epochs: 1
    verbose: false
    save_top_k: -1 # -1 -> all models are saved
    save_last: false # additionaly always save model from last epoch
    dirpath: "${hydra:runtime.output_dir}/checkpoints/"
    auto_insert_metric_name: false
