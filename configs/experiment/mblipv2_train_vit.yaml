# @package _global_
defaults:
  - default
  - /dataspecs@datamodule.train:
      - task_mix_llava_train
  - /dataspecs@datamodule.val:
      - task_mix_val_multigpu
  - override /module: llava
  - override /logger: csv #many_loggers

run:
  seed: 42
  data_prefix: /media/gregor/DATA/projects/wuerzburg
  image_root: /media/gregor/DATA/projects/wuerzburg/iglue/datasets
  train_image_root: ${run.image_root} #/images
  train_data: ${run.data_prefix}/lvlm-early-eval/data
  train_file: multilingual/combination/mblipv2_instruct_base_en.json  #multilingual/combination/llava_v2_mt.json
  xm3600_image_root: ${run.image_root}/Crossmodal3600/images
  flickr_image_root: ${run.image_root}/flickr30k/flickr_images
  gqa_image_root: ${run.image_root}/gqa/images
  mme_image_root: ${run.image_root}/MME
  mmbench_image_root: ${run.image_root}/mmbench/images
  vstar_image_root: ${run.image_root}/vstar_bench
  imagenet_image_root: ${run.image_root}/imagenet/train
  textvqa_image_root: ${run.image_root}/textvqa/train_val_images/train_images

  vit_model: vit_large_patch14_clip_336.openai #vit_so400m_patch14_siglip_384 #vit_large_patch14_clip_224.openai #vit_so400m_patch14_siglip_384
  llm:  microsoft/Phi-3.5-mini-instruct #meta-llama/Meta-Llama-3-8B-Instruct #lmsys/vicuna-7b-v1.5 #google/gemma-2-9b-it #stabilityai/stablelm-2-zephyr-1_6b #meta-llama/Meta-Llama-3-8B-Instruct  #lmsys/vicuna-7b-v1.5 #mistralai/Mistral-7B-Instruct-v0.2 #Qwen/Qwen1.5-4B-Chat #stabilityai/stablelm-2-zephyr-1_6b
  tokenizer_name: ${run.llm}
  max_seq_len: 512 #1024 #512 #768
  image_tokens: -1 #144 # 12Â²=144;  -1
  test_padding_side: "left" #"left"
  train_padding_side: "left" #"left"
#  train_padding_side: "left"
#  image_process_mode: "letterbox"
  test_batch_size: 8
  test_num_workers: 1
  train_batch_size: 1
  train_num_workers: 6
  train_shuffle: False

trainer:
#  limit_train_batches: 1.0
  limit_test_batches: 0.0
  max_epochs: 1
  devices: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  log_every_n_steps: 10
  val_check_interval: 0.25
  strategy:  auto #deepspeed_stage_2
#    _target_: src.tasks.vllm.checkpoint.MyDeepSpeedStrategy
#    stage: 2
#    config_kwargs:
#      data_types:
#        grad_accum_dtype: "fp32"
#      communication_data_type: "fp32"
#  limit_train_batches: 40
#  limit_val_batches: 3


module:
  _target_: src.modules.trident_module.llava_trident_module.LlavaTridentModule
  model:
#    _target_: src.modules.modeling.llava_module.LlavaUnslothModule
    lm_pretrained: ${run.llm}
    vit_pretrained: ${run.vit_model}
    train_checkpoint: null # or a checkpoint like: /example/path/to/runs/2023-05-13/11-46-24/checkpoints/0-8016.ckpt
    load_4bit: True
    load_8bit: False
    gradient_checkpoint: True
    use_flash_attn: True #True
    use_lora: True #True
    freeze_vit: False
    adapter_type: mlp  
    adapter_config:
      pool: 12 #14
      layers: 6
      ps_queries: 64
      multi_scale: "${oc.select:run.multiscale,2}"
    lora_alpha: 128
    lora_r: 64
    lora_dropout: 0.05
    overwrite_size: "${oc.select:run.overwrite_size,-1}"
  optimizer:
    lr: 0.0001
    vit_lr: 0.00001
    vit_weight_decay: 0.0
#    lora_lr: 0.0001  # llava: 0.001  perceiver: 0.0005 honeybee: 0.0003 mobilevlm: 0.001
    weight_decay: 0.0  # llava: 0 perceiver 0.0001 honeybee 0.0001 mobilevlm: 0
#    lora_lr: 0.0002
  scheduler:
    _target_: transformers.optimization.get_cosine_schedule_with_warmup
    num_warmup_steps: 0.03 # like llava

logger:
  csv:
    save_dir: ${hydra:runtime.output_dir}
  wandb:
    name: "instruct_${hydra:runtime.output_dir}"
    project: "mblipv2"


callbacks:
  model_checkpoint_on_epoch:
    _target_: src.tasks.vllm.checkpoint.LlavaModelCheckpoint #lightning.pytorch.callbacks.ModelCheckpoint
    lora: ${module.model.use_lora}
#    monitor: "" # name of the logged metric which determines when model is improving
    every_n_epochs: 1
    verbose: false
    save_top_k: -1 # -1 -> all models are saved
    save_last: false # additionaly always save model from last epoch
    dirpath: "${hydra:runtime.output_dir}/checkpoints/"
    auto_insert_metric_name: false
    save_weights_only: False
